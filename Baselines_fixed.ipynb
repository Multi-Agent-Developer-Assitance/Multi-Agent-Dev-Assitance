{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMI1yX3FpRgW",
    "outputId": "48e573d9-3459-42c9-8a38-ec4c9d1c162e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# === Step 0: Install dependencies ===\n",
    "!pip install transformers datasets torch tqdm\n",
    "\n",
    "# For Defect4j (Java), you need local setup, we will skip it for Colab simplicity\n",
    "# !apt-get install openjdk-11-jdk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qVdebNvpfEC"
   },
   "outputs": [],
   "source": [
    "# === Step 1: Import libraries ===\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from math import comb\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCKjSc2RqWMQ"
   },
   "outputs": [],
   "source": [
    "# === Step 2: Define pass@K calculation ===\n",
    "def compute_pass_at_k(n, c, k):\n",
    "    if c == 0:\n",
    "        return 0.0\n",
    "    if c >= n:\n",
    "        return 1.0\n",
    "    return 1 - comb(n-c, k)/comb(n, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348,
     "referenced_widgets": [
      "3165745cc5214742a7f62e5279a6ff2f",
      "236b9992e9414203a56a1a900f439040",
      "91c6b48095a743fca25bd6a14bb68f94",
      "1a8b2d9cbdec4af89c2014fa7f4f6d36",
      "3ac3134b092e4da6bb056d73bca5b499",
      "853f50c4973a47dfa9a3692d674274e1",
      "c3a12d27f3da4b858ae57a6905e56068",
      "3486047147ac48a083d0cb1da4201d88",
      "35169a4104224357a1b57cdc067826e6",
      "dacbe294d04643849deffa8bb9431757",
      "9bba09472dde43979cfec60271948512",
      "381849d4f3af4618a6e22d8fa02f5f16",
      "cf0b19a5203b4bd083f74feb211b3116",
      "6a575dc2889941da948c991ff5b32955",
      "3734b0dd860243da998fc3fb9f68eb20",
      "8dc7bd9f70654b439faa7b7f3bf87b23",
      "42fab8fcc9fb4a91a895c572ea5e4ee0",
      "54cc02adde4944558d613ac090735be8",
      "80084d071e9e475687a6056f9fe985e9",
      "6bea8a3c0592457bb07a100a8d0af651",
      "4dc5739245664696b5da49ac705af903",
      "ccac7a666bc345b788a279e118a5c126",
      "45fd6ff719954700ba932d606710e634",
      "7620536d68b0487692922ca029fa362f",
      "259711d7475a4cf6803e31fffc5425fc",
      "771f8ccb03c74cbc93806e3fd7b62db7",
      "71bf62305c174f6f9c06dce82c91c115",
      "ba8ffd841acc4b6ab635c1e4feda7db0",
      "7f8f5656a0fb47e8a0f30a683d1cd4e8",
      "38a1f39980064a3eb87f3def55552e00",
      "84b7cb362b36499fa1d6a54c673b2068",
      "9644ee678a854b48b85bfdc8e19700d6",
      "d2ca30f45f454cddb67d32933c1980d7",
      "de469b892a794480bf5fd962f290c60c",
      "00f46b2f612643f4a03eb8c0b9fadf2f",
      "a8ab46fd78ce4100b564847ea65d5e37",
      "2939beeed7ea4c07903fc8630542f968",
      "8fa2a2f71df7432fbef7bb1f488c53cb",
      "ec9532edb0924eaa88decffe0234d0c7",
      "07c510117904437ebde0b6242fa55d1c",
      "fff36dd6ebc5478faafcba4ba3b88d26",
      "c5ef6a64310a47bf9864b66c14a6d87a",
      "93e7b7c1455742c7b90a1ed044cea494",
      "a8c10c258f97407aba45e996d878d5a0",
      "f1e07098e6dd4bb3960e3c1439c15896",
      "f291acbf47a04e9eb2e832c2a263c34d",
      "d8ee87848f41459296745771220be2c6",
      "4e5e2ea8a1df4e909bb2bb89526c9ae1",
      "2ab4fe75195c4e52bbb0b4272aec8207",
      "cf1cadcc8b3a4e07be44c247b89a465b",
      "e1e2308ebf5d41ec8fd94e695180f068",
      "aaed23c0d4a54de2b99b42b62e6fb0d4",
      "64ea37f9572949f98bcb5403baa1c239",
      "00fae734a1ba498fbff976cca4f9dc3d",
      "42176eeeb4a448ba980ff3731aff59a6",
      "d84bb63d215f43a6b8dabc39987b573c",
      "d5507963c4454fbf94812cc8bb0b3dfa",
      "744e9ddd74fd4716aefb95e8c82c6159",
      "fff352d581304fd8a5acac1fd4e95504",
      "aa60423d9c4d49f1b7f83d252b9f2622",
      "791e8ba6de0c4aa3b9313ebb692f23f6",
      "30e1b44f2f6f4f48b2dfdb03d5b5a3e4",
      "c5bf65b52e86448ab08a032994e79ed4",
      "45de085011f34e68a2fe7d74cb1375cc",
      "30cd27f6873a44488b18251eeaf74661",
      "c0bd504b89ad4f9aaa8fbaa9f91aed5a",
      "a648b8d3895545cfa1dba004ab08658b",
      "ecadf9b12a104479ac850e1b02476685",
      "0dda0e7f0c0845fdb188c78e4688df57",
      "4affa617c2704e348eb429831b184ba8",
      "45f27f0965a94cd29e20b366b13492ca",
      "a848c361c6204609980e4e434933161c",
      "c9faeff49f884f4abd65c70aba24a51c",
      "d1e71022607d446fb331d96d1d3111e2",
      "e24f50398fbf4a88a1d2cabbe5be4f27",
      "59e7cfddd4c9482aa7059efae4635070",
      "0ed4f8b5c5f84553ae97c6fd60a6b2f4"
     ]
    },
    "id": "4nOJiuS_qaDy",
    "outputId": "610ceea8-2f31-428a-fc7a-64b2a327e4a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3165745cc5214742a7f62e5279a6ff2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381849d4f3af4618a6e22d8fa02f5f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45fd6ff719954700ba932d606710e634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de469b892a794480bf5fd962f290c60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e07098e6dd4bb3960e3c1439c15896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84bb63d215f43a6b8dabc39987b573c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a648b8d3895545cfa1dba004ab08658b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Step 3: Load Model ===\n",
    "# Choose model: TinyLlama or PHI-2\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"   # or \"microsoft/phi-2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")  # GPU if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hf6fyuumqf5a",
    "outputId": "7f6e70ee-f1cd-4bb9-f552-dbb07febe9ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanEval examples: 164\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "human_eval = load_dataset(\"openai_humaneval\")['test']\n",
    "\n",
    "print(f\"HumanEval examples: {len(human_eval)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DmpI5LrDq7pC",
    "outputId": "69fe8ebd-2800-4c04-922a-6e38765ecba1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/164 [35:38<21:42:31, 488.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 39/164 [6:38:09<23:29:11, 676.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcaefdg\n",
      "bcaefdgh\n",
      "bcaefdhigj\n",
      "bcaefdhigjk\n",
      "bcaefdg\n",
      "bcaefdgh\n",
      "bcaefdhigj\n",
      "bcaefdhigjk\n",
      "bcaefdg\n",
      "bcaefdgh\n",
      "bcaefdhigj\n",
      "bcaefdhigjk\n",
      "bcaefdg\n",
      "bcaefdgh\n",
      "bcaefdhigj\n",
      "bcaefdhigjk\n",
      "bcaefdg\n",
      "bcaefdgh\n",
      "bcaefdhigj\n",
      "bcaefdhigjk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 41/164 [7:01:56<23:49:32, 697.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 51/164 [8:45:21<19:24:01, 618.06s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3459310248.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Run on HumanEval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mhuman_eval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_humaneval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhuman_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-3459310248.py\u001b[0m in \u001b[0;36mrun_humaneval\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_SAMPLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_TOKENS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mcompletions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2787\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;34m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 459\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0;31m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         hidden_states, _ = self.self_attn(\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# === Step 5: Run dataset through model ===\n",
    "NUM_SAMPLES = 5       # n\n",
    "MAX_TOKENS = 200      # max tokens per generation\n",
    "\n",
    "def run_humaneval(dataset):\n",
    "    results = []\n",
    "    for item in tqdm(dataset):\n",
    "        prompt = item[\"prompt\"]\n",
    "        completions = []\n",
    "\n",
    "        for _ in range(NUM_SAMPLES):\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            output = model.generate(**inputs, max_new_tokens=MAX_TOKENS)\n",
    "            text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            completions.append(text)\n",
    "\n",
    "        # Evaluate correctness using test cases\n",
    "        correct_count = 0\n",
    "        for code in completions:\n",
    "            try:\n",
    "                exec(code)  # naive evaluation\n",
    "                correct_count += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # Compute pass@1, 5, 10\n",
    "        pass1 = compute_pass_at_k(NUM_SAMPLES, correct_count, 1)\n",
    "        pass5 = compute_pass_at_k(NUM_SAMPLES, correct_count, 5)\n",
    "        pass10 = compute_pass_at_k(NUM_SAMPLES, correct_count, 10)\n",
    "\n",
    "        results.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"completions\": completions,\n",
    "            \"correct_count\": correct_count,\n",
    "            \"pass1\": pass1,\n",
    "            \"pass5\": pass5,\n",
    "            \"pass10\": pass10\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Run on HumanEval\n",
    "human_eval_results = run_humaneval(human_eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "67c16337ef2f40a295d7a6deee625c21",
      "962c0ed2790444b7b0b01d9a3d8ec914",
      "aebbab4744d7414cbe5746c0db2e9c1c",
      "59ca8cebaa8f451995016ea19d5d427e",
      "04a11aa75bce43a5b626364b35abccde",
      "407749bb770a46a0a95080d910134da1",
      "723ca7f8f0fd43179f9adfff3c71d610",
      "350deb4b43c94744b5fe252271f79204",
      "ee16260b918844848be2b685de049572",
      "57647d3a896945b9ab37c0867c5829b6",
      "e7e6e2e1844d4d63a90433d7e377ceb0",
      "42967b35d3674dcdb0f30182ede3e823",
      "5da5285b6d604e67b579c2bd2995164f",
      "7b3d478fe19240efa99391e7c853036a",
      "8612a755039a49559c009068a9fff7be",
      "b36ff7cdc93346c8afe8471faccb0d25",
      "79d1ae1519424827bf121fe0306e68b1",
      "186e449b1c6a4188a5816aa62c5e73c3",
      "6c9b7c4b63c2469e9ffef47cd03bc572",
      "6024952f4910493d97077e642ce89210",
      "557a429d457b436ea8b9dd4b62704b16",
      "6f6c35af5cf545e0a7593fe0b1e14b49",
      "3b501771d4e349d1a174a6e1b1c8e25e",
      "09686a2272a446c1a5708706aca5e3b1",
      "b556b76a34314289a33ce0f836ef64b6",
      "e88de91e6081403493c39a1be705ee30",
      "21051d13dbfc4fda89307505d50f1273",
      "69084db3d08d483698e5e4365948d585",
      "c83c5ec06bbb40cc83aefb5eba794c9a",
      "34ba99cbb12b49648d4d5dc6a6be6d6a",
      "0c17774e55ac4d84954a18dcd552f41d",
      "3ce5a5efda1246b0934f011e104a5cc3",
      "5a79ff12600149b39f730bb6c025ad64",
      "9a0bb2cf0b3e439f8ca34e745d78d81b",
      "7e0fc8e72fd04bb59d97b6b932759b15",
      "5eb00a81d03f4f659418d99d0c899274",
      "9b2b7a3925f149cf885b8e657dfe56be",
      "95e13eeafbe5445ab1ac3348e29d39d7",
      "906dcc7a28464dda9a580c78c6224435",
      "6f4a48efbbf44bc68c31f29c6686bb1e",
      "c28dfd0f23964109a0122433d15d347e",
      "76925a7311e24068924a223ab628d09d",
      "7c1202aa23614cd88608ce1716352bf8",
      "dea4c8df16fe471e95e6d73a12f02bdb",
      "47983ea9edbb4209b92067000ccb3c38",
      "cc40ccb8b57849d984e2407511f8b7f6",
      "3625de92b8b1472b8e71f1e67dfe275e",
      "88061aa37f9145a8a3e817d528cc5dd3",
      "b666c41571d44cda9e6a9cc1a44023a8",
      "3ab40766724c492099bd9eb58c18dc7c",
      "3ec08f84d6334a64b54bc1db6cf38c4f",
      "f3f28664a98645899de0c0617a968c7e",
      "c49f3d07698c48a3bfe8097ee63cf4a3",
      "26fb2ca332d94024a0f126bf41402da4",
      "00e6ca825e774ef0b4199aee4a63a3aa",
      "91c8fbd1645e45da9fe90fb81b7abf9c",
      "66975c2bf0a74b64b5bc4679cf362ed1",
      "7a56cc908a004be3ac3b6d6eee998f6f",
      "f76948553f25423389b94cca1d6c47d1",
      "726077eec7784fee9e8d0f36506620ac",
      "954e416b6311400c829909a39512c922",
      "3367fc51e3984386bdc6c0bf6c84a6d1",
      "220bdf41d8404518aa7120043df4e394",
      "191ac43d286f4cfcbfc27b9197e43092",
      "c6ac741188414336802d317f583ad23d",
      "b65a85079b4e4f39aab38eb42cb366d0",
      "e5221ddc963f4d17baa5f04d0e1eb611",
      "a31a98ca42804368b040498aad9c6cfc",
      "f04164860c95470fa8a1423e4153a9ea",
      "70168fcd7e104bef98728fe4dc0d97b0",
      "f55ff86f4e1c48cf8ceddfce729f255d",
      "add9ac2a697944a8b05f65c3c7de6a2d",
      "d7a22070e81543fe888124b27e7dd31a",
      "0f331ed9731a4a6998bb756247b2d3d6",
      "254a989662e6409b8cc10c580ae26169",
      "b709bb6e73ab48b8ade6c8d1cbb67299",
      "b7994d67a4fe4a84b1a65b79f23d2340",
      "008a4b26b2794d70a621fbd05bdcc7b0",
      "0a6e4ac4c2084c56804ad991525348b7",
      "61eec2f206c7461c8a48bd0350d2e2b9",
      "0751097a241443eba12c69a20609cc1a",
      "2af022b700db4867a17cf0f50d5ad406",
      "efd1c379791746c58a1be88ad171caa9",
      "146288c997c34d1c9109ea0781b54a0a",
      "479db5411484486d8fe61bb65ac9aa9d",
      "40b21917d33c46fe99ad067685aa1ce2",
      "0e0071e43cc241d1a61a33148014a676",
      "a9031765f70d45dbb634d7c08409c3a3",
      "1e5f7b01beee41a4b793de31bd4a2629",
      "df011eebc47b4abaacbc2e733d283f32",
      "eeb57b2df5644769844e315f1eb2c4b1",
      "0932fbe9856445ea96e8f68a8246566a",
      "02f188b5685c48d3bc9851b931d575d6",
      "2c88dc89baf8425ba348a64c2720d4cb",
      "380c926cefb1454d93d62ef68e2a96ef",
      "39ce7ec1c47948e0b666d2449a37e3cb",
      "fe70cb0a291e44b5b8768fc168952810",
      "ca80d3ba29e1488283812d1d6de63474",
      "5528cdcb09384cda9c17666f3174904a",
      "1e0e4b6c38e34a54ba6f3157c402ad47",
      "1deb562450ed4b92b7bbcd21bc52202d",
      "86c2c8f0e61b4cd1ba34cf4ca1b37990",
      "754ba576b64646a682708524b218990f",
      "162cb5f97dac45e092297e19ef7dc920",
      "33c2688ce1a843b082a965538e5ff99d",
      "2acc2734a478442b9abb76976a4a6d95",
      "ee5690e233074ecf8598e7f5d666098c",
      "4457b18d6e604696b1940df608caef42",
      "7afb59c4ca1e44789a752147a29fe0ed",
      "e47a904f03ee4d8a90f4f83750027f46"
     ]
    },
    "id": "ip1skIx0lnx_",
    "outputId": "cbe1d5a4-3ac8-468f-e497-c6d7e400319b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG: TinyLlama/TinyLlama-1.1B-Chat-v1.0 NUM_SAMPLES= 5 BATCH_SIZE= 8 DEVICE= cuda\n",
      "Loading HumanEval dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c16337ef2f40a295d7a6deee625c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42967b35d3674dcdb0f30182ede3e823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openai_humaneval/test-00000-of-00001.par(…):   0%|          | 0.00/83.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b501771d4e349d1a174a6e1b1c8e25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/164 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanEval size: 164\n",
      "Loading model... (this may take a minute)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0bb2cf0b3e439f8ca34e745d78d81b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47983ea9edbb4209b92067000ccb3c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c8fbd1645e45da9fe90fb81b7abf9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5221ddc963f4d17baa5f04d0e1eb611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008a4b26b2794d70a621fbd05bdcc7b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5f7b01beee41a4b793de31bd4a2629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0e4b6c38e34a54ba6f3157c402ad47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. Device example param: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  5%|▍         | 1/21 [00:10<03:26, 10.34s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 10%|▉         | 2/21 [00:19<03:03,  9.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 14%|█▍        | 3/21 [00:28<02:51,  9.51s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 19%|█▉        | 4/21 [00:37<02:35,  9.17s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 24%|██▍       | 5/21 [00:49<02:40, 10.05s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 29%|██▊       | 6/21 [00:59<02:30, 10.06s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 33%|███▎      | 7/21 [01:08<02:17,  9.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 38%|███▊      | 8/21 [01:17<02:06,  9.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 43%|████▎     | 9/21 [01:30<02:08, 10.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 48%|████▊     | 10/21 [01:43<02:05, 11.45s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 52%|█████▏    | 11/21 [01:56<01:56, 11.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 57%|█████▋    | 12/21 [02:08<01:48, 12.00s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 62%|██████▏   | 13/21 [02:18<01:30, 11.27s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 67%|██████▋   | 14/21 [02:31<01:21, 11.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 71%|███████▏  | 15/21 [02:44<01:12, 12.11s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 76%|███████▌  | 16/21 [02:56<01:00, 12.07s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 81%|████████  | 17/21 [03:11<00:51, 12.90s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 86%|████████▌ | 18/21 [03:21<00:36, 12.24s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 90%|█████████ | 19/21 [03:32<00:23, 11.84s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 95%|█████████▌| 20/21 [03:45<00:12, 12.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|██████████| 21/21 [03:52<00:00, 11.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation done in 3.87 minutes. 164 tasks processed.\n",
      "SUMMARY: {'tasks': 164, 'avg_pass@1': np.float64(0.28292682926829266), 'avg_pass@5': np.float64(0.6951219512195121), 'avg_pass@10': np.float64(0.6951219512195121)}\n",
      "Saved per-task results to TinyLlama_TinyLlama-1.1B-Chat-v1.0_humaneval_results.csv\n",
      "Saved summary (print above).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Colab-ready efficient HumanEval evaluation (batching + safe subprocess eval + pass@K) ===\n",
    "# Paste and run in a single Colab cell.\n",
    "\n",
    "# 0) Install dependencies (only first run)\n",
    "!pip install -q transformers datasets accelerate torch tqdm\n",
    "\n",
    "# 1) Imports\n",
    "import os, json, math, tempfile, subprocess, sys, time\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "# 2) Config - change model name here to test other models\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"   # or \"microsoft/phi-2\"\n",
    "NUM_SAMPLES = 5        # n (number of completions per prompt)\n",
    "MAX_NEW_TOKENS = 128   # length of generated code (tokens)\n",
    "BATCH_SIZE = 8         # prompts per batch (adjust upward if you have more GPU RAM)\n",
    "TEMPERATURE = 0.8\n",
    "TOP_P = 0.95\n",
    "EVAL_TIMEOUT = 4       # seconds allowed per generated completion (timeout for tests)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"CONFIG:\", MODEL_NAME, \"NUM_SAMPLES=\", NUM_SAMPLES, \"BATCH_SIZE=\", BATCH_SIZE, \"DEVICE=\", DEVICE)\n",
    "\n",
    "# 3) Safe evaluation: run code + test in subprocess with timeout\n",
    "def eval_code_with_test(code_str: str, test_str: str, timeout: int = EVAL_TIMEOUT) -> bool:\n",
    "    \"\"\"\n",
    "    Executes code_str followed by test_str in a subprocess python -c '...'\n",
    "    Returns True if tests pass (exit code 0), False otherwise or on timeout.\n",
    "    This isolates execution and avoids hanging the main process.\n",
    "    \"\"\"\n",
    "    # combine into one snippet\n",
    "    # We ensure abort on assertion failure by not swallowing exceptions.\n",
    "    combined = code_str + \"\\n\\n\" + test_str + \"\\n\"\n",
    "    # Use a temporary file approach to avoid shell quoting issues\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as tf:\n",
    "        tf.write(combined)\n",
    "        fname = tf.name\n",
    "    try:\n",
    "        # run the temp file as a separate Python process\n",
    "        proc = subprocess.run([sys.executable, fname],\n",
    "                              stdout=subprocess.PIPE,\n",
    "                              stderr=subprocess.PIPE,\n",
    "                              timeout=timeout)\n",
    "        success = proc.returncode == 0\n",
    "        # optional: debug prints on failure can be enabled\n",
    "        # if not success: print(proc.stderr.decode()[:300])\n",
    "        return success\n",
    "    except subprocess.TimeoutExpired:\n",
    "        # timed out -> treat as failure\n",
    "        return False\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(fname)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# 4) pass@k calculator\n",
    "def pass_at_k(n:int, c:int, k:int) -> float:\n",
    "    if c == 0:\n",
    "        return 0.0\n",
    "    if k > n:\n",
    "        k = n\n",
    "    # Use math.comb; handle edge cases\n",
    "    try:\n",
    "        return 1.0 - math.comb(n - c, k) / math.comb(n, k)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "# 5) Load dataset (HumanEval has 'test' split)\n",
    "print(\"Loading HumanEval dataset...\")\n",
    "humaneval = load_dataset(\"openai_humaneval\")[\"test\"]\n",
    "print(\"HumanEval size:\", len(humaneval))\n",
    "\n",
    "# 6) Load model + tokenizer (FP16 where possible). Use device_map=\"auto\" to let transformers handle device placement.\n",
    "print(\"Loading model... (this may take a minute)\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "# Try to load with fp16 if GPU present\n",
    "if DEVICE == \"cuda\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "print(\"Model loaded. Device example param:\", next(model.parameters()).device)\n",
    "\n",
    "# 7) Batched generation + evaluation loop\n",
    "def evaluate_model_on_humaneval(dataset, tokenizer, model,\n",
    "                                batch_size=BATCH_SIZE, num_samples=NUM_SAMPLES,\n",
    "                                max_new_tokens=MAX_NEW_TOKENS):\n",
    "    results = []  # list of dicts with task_id, c, pass@1,5,10\n",
    "    n_total = len(dataset)\n",
    "    # Precompute tasks as (task_id, prompt, test)\n",
    "    tasks = []\n",
    "    for item in dataset:\n",
    "        # HumanEval entries typically contain 'prompt' and 'test' keys (test is string with assertions)\n",
    "        prompt = item.get(\"prompt\") or item.get(\"code\") or \"\"\n",
    "        test = item.get(\"test\") or item.get(\"ref\") or \"\"\n",
    "        task_id = item.get(\"task_id\", None) or item.get(\"id\", None) or len(tasks)\n",
    "        tasks.append((task_id, prompt, test))\n",
    "\n",
    "    # iterate in batches\n",
    "    for i in tqdm(range(0, n_total, batch_size)):\n",
    "        batch = tasks[i:i+batch_size]\n",
    "        prompts = [p for (_id, p, t) in batch]\n",
    "        # tokenize batch with padding\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(next(model.parameters()).device)\n",
    "\n",
    "        # generate num_return_sequences per prompt in one call\n",
    "        # transformers will return (batch_size * num_return_sequences) outputs in order\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=True,\n",
    "                temperature=TEMPERATURE,\n",
    "                top_p=TOP_P,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                num_return_sequences=num_samples,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # decode all outputs\n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        # decoded is list length batch_size * num_samples, grouped: for prompt0 -> r0..rN, prompt1 -> r0..rN, ...\n",
    "        # process each prompt group\n",
    "        for idx_in_batch, (task_id, prompt, test_str) in enumerate(batch):\n",
    "            # collect completions for this prompt\n",
    "            start = idx_in_batch * num_samples\n",
    "            end = start + num_samples\n",
    "            completions = decoded[start:end]\n",
    "\n",
    "            # Evaluate each completion safely\n",
    "            correct_count = 0\n",
    "            for code in completions:\n",
    "                # Many models return the prompt + completion, so try to extract only the generated suffix:\n",
    "                # If the prompt is included in the model output, remove it.\n",
    "                out = code\n",
    "                if out.startswith(prompt):\n",
    "                    out = out[len(prompt):].lstrip(\"\\n\")\n",
    "                # Combine prompt + generated content to form a complete file\n",
    "                full_code = prompt + \"\\n\" + out\n",
    "                # Run test in isolated subprocess, with timeout\n",
    "                passed = False\n",
    "                if test_str and len(test_str.strip())>0:\n",
    "                    passed = eval_code_with_test(full_code, test_str, timeout=EVAL_TIMEOUT)\n",
    "                else:\n",
    "                    # If no test available, do a light heuristic: check \"return\" or \"def\"\n",
    "                    passed = (\"return\" in out) or (\"def \" in out)\n",
    "                if passed:\n",
    "                    correct_count += 1\n",
    "\n",
    "            # compute pass@k for k in {1,5,10}\n",
    "            p1 = pass_at_k(num_samples, correct_count, 1)\n",
    "            p5 = pass_at_k(num_samples, correct_count, 5)\n",
    "            p10 = pass_at_k(num_samples, correct_count, 10)\n",
    "\n",
    "            results.append({\n",
    "                \"task_id\": task_id,\n",
    "                \"correct_count\": correct_count,\n",
    "                \"num_samples\": num_samples,\n",
    "                \"pass@1\": p1,\n",
    "                \"pass@5\": p5,\n",
    "                \"pass@10\": p10\n",
    "            })\n",
    "\n",
    "        # small sleep to avoid GPU throttling issues\n",
    "        time.sleep(0.05)\n",
    "\n",
    "    return results\n",
    "\n",
    "# 8) Run evaluation\n",
    "start_time = time.time()\n",
    "results = evaluate_model_on_humaneval(humaneval, tokenizer, model,\n",
    "                                      batch_size=BATCH_SIZE,\n",
    "                                      num_samples=NUM_SAMPLES,\n",
    "                                      max_new_tokens=MAX_NEW_TOKENS)\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Evaluation done in {elapsed/60:.2f} minutes. {len(results)} tasks processed.\")\n",
    "\n",
    "# 9) Summarize and save\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "summary = {\n",
    "    \"tasks\": len(df),\n",
    "    \"avg_pass@1\": df[\"pass@1\"].mean(),\n",
    "    \"avg_pass@5\": df[\"pass@5\"].mean(),\n",
    "    \"avg_pass@10\": df[\"pass@10\"].mean()\n",
    "}\n",
    "print(\"SUMMARY:\", summary)\n",
    "\n",
    "out_csv = f\"{MODEL_NAME.replace('/', '_')}_humaneval_results.csv\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "print(\"Saved per-task results to\", out_csv)\n",
    "print(\"Saved summary (print above).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HWd3uBfTpovO",
    "outputId": "6c0f20a9-caa8-4026-979f-111715c3b28f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG: TinyLlama/TinyLlama-1.1B-Chat-v1.0 NUM_SAMPLES= 5 BATCH_SIZE= 8 DEVICE= cuda\n",
      "Loading HumanEval (official)...\n",
      "HumanEval size: 164\n",
      "No local humaneval_plus.json found. Creating a tiny MOCK HumanEval+ (2 examples) for testing the pipeline.\n",
      "Loading tokenizer & model (may take a minute)...\n",
      "Model loaded on: cuda:0\n",
      "\n",
      "=== Running evaluation on HumanEval+ ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 0.11 minutes — processed 2 problems.\n",
      "SUMMARY: {'dataset': 'HumanEval+ (mock)', 'tasks': 2, 'avg_pass@1': np.float64(0.2), 'avg_pass@5': np.float64(0.5), 'avg_pass@10': np.float64(0.5)}\n",
      "Saved results to TinyLlama_TinyLlama-1.1B-Chat-v1.0_humaneval_plus_results.csv\n",
      "\n",
      "Note: You ran a mock HumanEval+. Replace 'humaneval_plus.json' with the real dataset to evaluate fully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Colab-ready: Evaluate HumanEval+ (Testing) on TinyLlama ===\n",
    "# Paste this entire cell into Colab and run.\n",
    "\n",
    "# 0) Install deps (first-run)\n",
    "!pip install -q transformers datasets accelerate torch tqdm\n",
    "\n",
    "# 1) Imports\n",
    "import os, json, math, tempfile, subprocess, sys, time\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# 2) Config — EDIT if desired\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # model to evaluate\n",
    "NUM_SAMPLES = 5            # n: completions per prompt (increase for better stats)\n",
    "MAX_NEW_TOKENS = 200       # tokens per generation\n",
    "BATCH_SIZE = 8             # prompts per batch (adjust to GPU mem)\n",
    "TEMPERATURE = 0.8\n",
    "TOP_P = 0.95\n",
    "EVAL_TIMEOUT = 4           # seconds per generated completion (subprocess timeout)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"CONFIG:\", MODEL_NAME, \"NUM_SAMPLES=\", NUM_SAMPLES, \"BATCH_SIZE=\", BATCH_SIZE, \"DEVICE=\", DEVICE)\n",
    "\n",
    "# 3) Safe evaluation helper (subprocess, timeout)\n",
    "def eval_code_with_test(code_str: str, test_str: str, timeout: int = EVAL_TIMEOUT) -> bool:\n",
    "    \"\"\"\n",
    "    Runs combined code_str + test_str in a separate Python process with timeout.\n",
    "    Returns True if process exits 0 (tests passed), False otherwise or on timeout.\n",
    "    \"\"\"\n",
    "    combined = code_str + \"\\n\\n\" + test_str + \"\\n\"\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as tf:\n",
    "        tf.write(combined)\n",
    "        fname = tf.name\n",
    "    try:\n",
    "        proc = subprocess.run([sys.executable, fname],\n",
    "                              stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n",
    "                              timeout=timeout)\n",
    "        return proc.returncode == 0\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return False\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(fname)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# 4) pass@K\n",
    "def pass_at_k(n:int, c:int, k:int) -> float:\n",
    "    if c == 0:\n",
    "        return 0.0\n",
    "    if k > n:\n",
    "        k = n\n",
    "    try:\n",
    "        return 1.0 - math.comb(n - c, k) / math.comb(n, k)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "# 5) Load datasets: HumanEval (HF) + local HumanEval+ (or mock)\n",
    "print(\"Loading HumanEval (official)...\")\n",
    "human_eval = load_dataset(\"openai_humaneval\")[\"test\"]\n",
    "print(\"HumanEval size:\", len(human_eval))\n",
    "\n",
    "# Try to load local 'humaneval_plus.json' (user-provided). If not found, create a tiny mock file.\n",
    "HE_PLUS_FILE = \"humaneval_plus.json\"\n",
    "if os.path.exists(HE_PLUS_FILE):\n",
    "    print(\"Loading local HumanEval+ from\", HE_PLUS_FILE)\n",
    "    with open(HE_PLUS_FILE, \"r\") as f:\n",
    "        humaneval_plus = json.load(f)\n",
    "    is_mock = False\n",
    "    print(\"HumanEval+ loaded:\", len(humaneval_plus), \"examples\")\n",
    "else:\n",
    "    print(\"No local humaneval_plus.json found. Creating a tiny MOCK HumanEval+ (2 examples) for testing the pipeline.\")\n",
    "    humaneval_plus = [\n",
    "        {\n",
    "            \"prompt\": \"def add(a, b):\\n    \\\"\\\"\\\"Return sum of a and b.\\\"\\\"\\\"\\n\",\n",
    "            \"test\": \"assert add(2,3) == 5\\nassert add(-1,1) == 0\\n\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"def factorial(n):\\n    \\\"\\\"\\\"Return factorial of n (n>=0).\\\"\\\"\\\"\\n\",\n",
    "            \"test\": \"assert factorial(0) == 1\\nassert factorial(5) == 120\\n\"\n",
    "        }\n",
    "    ]\n",
    "    is_mock = True\n",
    "\n",
    "# 6) Load model + tokenizer (FP16 if GPU)\n",
    "print(\"Loading tokenizer & model (may take a minute)...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if DEVICE == \"cuda\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "print(\"Model loaded on:\", next(model.parameters()).device)\n",
    "\n",
    "# 7) Evaluation loop (batched, uses num_return_sequences)\n",
    "def evaluate_humaneval_plus(entries, batch_size=BATCH_SIZE, num_samples=NUM_SAMPLES, max_new_tokens=MAX_NEW_TOKENS):\n",
    "    tasks = [(idx, e.get(\"prompt\",\"\"), e.get(\"test\",\"\")) for idx,e in enumerate(entries)]\n",
    "    results = []\n",
    "    n_total = len(tasks)\n",
    "    for i in tqdm(range(0, n_total, batch_size), desc=\"Batches\"):\n",
    "        batch = tasks[i:i+batch_size]\n",
    "        prompts = [p for (_id,p,t) in batch]\n",
    "        # Tokenize once for the batch\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(next(model.parameters()).device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=True,\n",
    "                temperature=TEMPERATURE,\n",
    "                top_p=TOP_P,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                num_return_sequences=num_samples,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # decode outputs as list length batch_size * num_samples\n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        for bi, (task_id, prompt, test_str) in enumerate(batch):\n",
    "            start = bi * num_samples\n",
    "            end = start + num_samples\n",
    "            completions = decoded[start:end]\n",
    "            correct_count = 0\n",
    "            for comp in completions:\n",
    "                out = comp\n",
    "                # If model echoed prompt, strip it\n",
    "                if out.startswith(prompt):\n",
    "                    out = out[len(prompt):].lstrip(\"\\n\")\n",
    "                full_code = prompt + \"\\n\" + out\n",
    "                passed = False\n",
    "                if test_str and test_str.strip():\n",
    "                    passed = eval_code_with_test(full_code, test_str, timeout=EVAL_TIMEOUT)\n",
    "                else:\n",
    "                    # fallback heuristic if no tests: check presence of 'return' or 'def'\n",
    "                    passed = (\"return\" in out) or (\"def \" in out)\n",
    "                if passed:\n",
    "                    correct_count += 1\n",
    "\n",
    "            results.append({\n",
    "                \"task_id\": task_id,\n",
    "                \"correct_count\": correct_count,\n",
    "                \"num_samples\": num_samples,\n",
    "                \"pass@1\": pass_at_k(num_samples, correct_count, 1),\n",
    "                \"pass@5\": pass_at_k(num_samples, correct_count, 5),\n",
    "                \"pass@10\": pass_at_k(num_samples, correct_count, 10)\n",
    "            })\n",
    "    return results\n",
    "\n",
    "# 8) Run evaluation on HumanEval+ (the local file or mock)\n",
    "print(\"\\n=== Running evaluation on HumanEval+ ===\")\n",
    "start = time.time()\n",
    "he_plus_results = evaluate_humaneval_plus(humaneval_plus, batch_size=BATCH_SIZE, num_samples=NUM_SAMPLES, max_new_tokens=MAX_NEW_TOKENS)\n",
    "elapsed = time.time() - start\n",
    "print(f\"Done in {elapsed/60:.2f} minutes — processed {len(he_plus_results)} problems.\")\n",
    "\n",
    "# 9) Summarize & save\n",
    "df_he_plus = pd.DataFrame(he_plus_results)\n",
    "summary = {\n",
    "    \"dataset\": \"HumanEval+ (mock)\" if is_mock else \"HumanEval+ (local)\",\n",
    "    \"tasks\": len(df_he_plus),\n",
    "    \"avg_pass@1\": df_he_plus[\"pass@1\"].mean(),\n",
    "    \"avg_pass@5\": df_he_plus[\"pass@5\"].mean(),\n",
    "    \"avg_pass@10\": df_he_plus[\"pass@10\"].mean()\n",
    "}\n",
    "print(\"SUMMARY:\", summary)\n",
    "\n",
    "out_csv = f\"{MODEL_NAME.replace('/','_')}_humaneval_plus_results.csv\"\n",
    "df_he_plus.to_csv(out_csv, index=False)\n",
    "print(\"Saved results to\", out_csv)\n",
    "\n",
    "# 10) Also (optional) run on official HumanEval (HF) and save side-by-side for comparison\n",
    "if not is_mock:\n",
    "    print(\"\\nNote: You provided a local HumanEval+; run official HumanEval separately if you want full comparison.\")\n",
    "else:\n",
    "    print(\"\\nNote: You ran a mock HumanEval+. Replace 'humaneval_plus.json' with the real dataset to evaluate fully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gkp0xS8a-nNa",
    "outputId": "66f53c73-fd63-4adc-a3a2-47e84c4aeb5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TinyLlama/TinyLlama-1.1B-Chat-v1.0 on cuda...\n",
      "✅ Model loaded successfully.\n",
      "Loading HumanEval+ dataset (evalplus/humanevalplus)...\n",
      "✅ Loaded 164 tasks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/41 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   2%|▏         | 1/41 [00:11<07:35, 11.38s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   5%|▍         | 2/41 [00:22<07:12, 11.08s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   7%|▋         | 3/41 [00:35<07:32, 11.91s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  10%|▉         | 4/41 [00:46<07:06, 11.51s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  12%|█▏        | 5/41 [01:01<07:46, 12.96s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  15%|█▍        | 6/41 [01:14<07:28, 12.83s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  17%|█▋        | 7/41 [01:25<06:57, 12.27s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  20%|█▉        | 8/41 [01:36<06:35, 11.99s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  22%|██▏       | 9/41 [01:52<07:03, 13.23s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  24%|██▍       | 10/41 [02:04<06:39, 12.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  27%|██▋       | 11/41 [02:17<06:25, 12.83s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  29%|██▉       | 12/41 [02:29<06:05, 12.60s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  32%|███▏      | 13/41 [02:41<05:45, 12.35s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  34%|███▍      | 14/41 [02:53<05:32, 12.30s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  37%|███▋      | 15/41 [03:04<05:12, 12.03s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  39%|███▉      | 16/41 [03:17<05:06, 12.26s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  41%|████▏     | 17/41 [03:32<05:11, 12.98s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  44%|████▍     | 18/41 [03:50<05:34, 14.54s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  46%|████▋     | 19/41 [04:05<05:24, 14.75s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  49%|████▉     | 20/41 [04:23<05:31, 15.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  51%|█████     | 21/41 [04:39<05:15, 15.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  54%|█████▎    | 22/41 [04:55<04:59, 15.75s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  56%|█████▌    | 23/41 [05:08<04:31, 15.11s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  59%|█████▊    | 24/41 [05:26<04:31, 15.94s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  61%|██████    | 25/41 [05:39<03:59, 14.98s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  63%|██████▎   | 26/41 [05:52<03:33, 14.26s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  66%|██████▌   | 27/41 [06:08<03:29, 14.98s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  68%|██████▊   | 28/41 [06:26<03:26, 15.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  71%|███████   | 29/41 [06:44<03:18, 16.56s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  73%|███████▎  | 30/41 [06:58<02:53, 15.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  76%|███████▌  | 31/41 [07:16<02:41, 16.16s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  78%|███████▊  | 32/41 [07:31<02:24, 16.06s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  80%|████████  | 33/41 [07:51<02:17, 17.20s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  83%|████████▎ | 34/41 [08:04<01:50, 15.79s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  85%|████████▌ | 35/41 [08:17<01:29, 14.91s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  88%|████████▊ | 36/41 [08:31<01:13, 14.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  90%|█████████ | 37/41 [08:44<00:56, 14.19s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  93%|█████████▎| 38/41 [08:59<00:43, 14.38s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  95%|█████████▌| 39/41 [09:15<00:30, 15.01s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  98%|█████████▊| 40/41 [09:32<00:15, 15.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches: 100%|██████████| 41/41 [09:47<00:00, 14.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Summary: {'tasks': 164, 'avg_pass@1': np.float64(0.2670731707317073), 'avg_pass@5': np.float64(0.7439024390243902), 'avg_pass@10': np.float64(0.7439024390243902)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Step 0: Install dependencies (first run only) ===\n",
    "!pip install -q transformers datasets torch tqdm pandas\n",
    "\n",
    "# === Step 1: Imports ===\n",
    "import torch, math, time, subprocess, sys, tempfile\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG\n",
    "# =========================================================\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 4\n",
    "NUM_SAMPLES = 5\n",
    "MAX_NEW_TOKENS = 128\n",
    "TEMPERATURE = 0.8\n",
    "TOP_P = 0.95\n",
    "EVAL_TIMEOUT = 2  # seconds per completion\n",
    "\n",
    "# =========================================================\n",
    "# Step 2: Load model & tokenizer\n",
    "# =========================================================\n",
    "print(f\"Loading {MODEL_NAME} on {DEVICE}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model.eval()\n",
    "print(\"✅ Model loaded successfully.\")\n",
    "\n",
    "# =========================================================\n",
    "# Step 3: Load HumanEval+ dataset (EvalPlus)\n",
    "# =========================================================\n",
    "print(\"Loading HumanEval+ dataset (evalplus/humanevalplus)...\")\n",
    "dataset = load_dataset(\"evalplus/humanevalplus\")[\"test\"]\n",
    "entries = list(dataset)  # convert to list of dicts\n",
    "print(f\"✅ Loaded {len(entries)} tasks.\")\n",
    "\n",
    "# =========================================================\n",
    "# Step 4: Helper functions\n",
    "# =========================================================\n",
    "def pass_at_k(n:int, c:int, k:int) -> float:\n",
    "    if c == 0: return 0.0\n",
    "    if k > n: k = n\n",
    "    try:\n",
    "        return 1.0 - math.comb(n-c, k)/math.comb(n, k)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "def safe_eval_subprocess(code:str, test:str, timeout:int=EVAL_TIMEOUT) -> bool:\n",
    "    \"\"\"Execute code+test safely in a subprocess with timeout.\"\"\"\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as tf:\n",
    "        tf.write(code + \"\\n\" + test)\n",
    "        fname = tf.name\n",
    "    try:\n",
    "        proc = subprocess.run([sys.executable, fname],\n",
    "                              stdout=subprocess.PIPE,\n",
    "                              stderr=subprocess.PIPE,\n",
    "                              timeout=timeout)\n",
    "        return proc.returncode == 0\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return False\n",
    "    finally:\n",
    "        try:\n",
    "            import os; os.remove(fname)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# =========================================================\n",
    "# Step 5: Batched evaluation\n",
    "# =========================================================\n",
    "results = []\n",
    "num_tasks = len(entries)\n",
    "\n",
    "for i in tqdm(range(0, num_tasks, BATCH_SIZE), desc=\"Batches\"):\n",
    "    batch = entries[i:i+BATCH_SIZE]\n",
    "    prompts = [entry[\"prompt\"] for entry in batch]\n",
    "    tests = [entry[\"test\"] for entry in batch]\n",
    "    task_ids = [entry[\"task_id\"] for entry in batch]\n",
    "\n",
    "    # Tokenize batch\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "\n",
    "    # Generate NUM_SAMPLES per prompt\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=True,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            num_return_sequences=NUM_SAMPLES,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    # Assign outputs to prompts\n",
    "    for idx, (prompt, test, task_id) in enumerate(zip(prompts, tests, task_ids)):\n",
    "        completions = decoded[idx*NUM_SAMPLES:(idx+1)*NUM_SAMPLES]\n",
    "        correct_count = 0\n",
    "        for c in completions:\n",
    "            # remove prompt from completion if repeated\n",
    "            if c.startswith(prompt):\n",
    "                c = c[len(prompt):].lstrip(\"\\n\")\n",
    "            full_code = prompt + \"\\n\" + c\n",
    "            if safe_eval_subprocess(full_code, test):\n",
    "                correct_count += 1\n",
    "        results.append({\n",
    "            \"task_id\": task_id,\n",
    "            \"correct_count\": correct_count,\n",
    "            \"pass@1\": pass_at_k(NUM_SAMPLES, correct_count, 1),\n",
    "            \"pass@5\": pass_at_k(NUM_SAMPLES, correct_count, 5),\n",
    "            \"pass@10\": pass_at_k(NUM_SAMPLES, correct_count, 10)\n",
    "        })\n",
    "\n",
    "# =========================================================\n",
    "# Step 6: Save results\n",
    "# =========================================================\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(f\"{MODEL_NAME.replace('/', '_')}_humanevalplus_results.csv\", index=False)\n",
    "summary = {\n",
    "    \"tasks\": len(df),\n",
    "    \"avg_pass@1\": df[\"pass@1\"].mean(),\n",
    "    \"avg_pass@5\": df[\"pass@5\"].mean(),\n",
    "    \"avg_pass@10\": df[\"pass@10\"].mean()\n",
    "}\n",
    "print(\"✅ Evaluation done. Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eOnJPsJMCcP7",
    "outputId": "adb60796-5c9a-4463-d0f2-761a07edd625"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG: TinyLlama/TinyLlama-1.1B-Chat-v1.0 | DEVICE=cuda | BATCH=4 | NUM_SAMPLES=5 | PlanBench config=task_1_plan_generation\n",
      "✅ Model loaded successfully.\n",
      "✅ Loaded 2270 tasks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 568/568 [37:49<00:00,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY: {'tasks': 2270, 'avg_pass@1': np.float64(0.053568281938325975), 'avg_pass@5': np.float64(0.23832599118942732), 'avg_pass@10': np.float64(0.23832599118942732)}\n",
      "✅ Saved per-task results to TinyLlama_TinyLlama-1.1B-Chat-v1.0_task_1_plan_generation_planbench_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Colab-ready PlanBench evaluation on TinyLlama ===\n",
    "\n",
    "!pip install -q transformers datasets torch tqdm accelerate\n",
    "\n",
    "import torch, math, time\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Config\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 4\n",
    "NUM_SAMPLES = 5\n",
    "MAX_NEW_TOKENS = 128\n",
    "TEMPERATURE = 0.8\n",
    "TOP_P = 0.95\n",
    "\n",
    "CONFIG_NAME = \"task_1_plan_generation\"  # choose PlanBench config here\n",
    "print(f\"CONFIG: {MODEL_NAME} | DEVICE={DEVICE} | BATCH={BATCH_SIZE} | NUM_SAMPLES={NUM_SAMPLES} | PlanBench config={CONFIG_NAME}\")\n",
    "\n",
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if DEVICE==\"cuda\" else None,\n",
    "    device_map=\"auto\" if DEVICE==\"cuda\" else None\n",
    ")\n",
    "model.eval()\n",
    "print(\"✅ Model loaded successfully.\")\n",
    "\n",
    "# Load PlanBench dataset\n",
    "dataset = load_dataset(\"tasksource/planbench\", CONFIG_NAME)[\"train\"]\n",
    "print(f\"✅ Loaded {len(dataset)} tasks.\")\n",
    "\n",
    "# pass@k function\n",
    "def pass_at_k(n:int, c:int, k:int) -> float:\n",
    "    if c == 0: return 0.0\n",
    "    if k > n: k = n\n",
    "    try:\n",
    "        return 1.0 - math.comb(n - c, k) / math.comb(n, k)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "results = []\n",
    "n_total = len(dataset)\n",
    "dataset_list = dataset.to_list()  # convert to list of dicts\n",
    "\n",
    "# Batched evaluation\n",
    "for i in tqdm(range(0, n_total, BATCH_SIZE), desc=\"Batches\"):\n",
    "    batch = dataset_list[i:i+BATCH_SIZE]\n",
    "\n",
    "    # Dynamically extract a prompt from available fields\n",
    "    prompts = [item.get(\"task_description\") or item.get(\"task\") or item.get(\"description\") or \"\" for item in batch]\n",
    "    task_ids = [item.get(\"task_id\", idx+i) for idx,item in enumerate(batch)]\n",
    "\n",
    "    # tokenize batch\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            num_return_sequences=NUM_SAMPLES,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    # process completions\n",
    "    for idx_in_batch, prompt in enumerate(prompts):\n",
    "        start = idx_in_batch * NUM_SAMPLES\n",
    "        end = start + NUM_SAMPLES\n",
    "        completions = decoded[start:end]\n",
    "\n",
    "        # simple heuristic: check for \"action\" or \"goal\"\n",
    "        correct_count = sum(1 for c in completions if \"action\" in c.lower() or \"goal\" in c.lower())\n",
    "\n",
    "        p1 = pass_at_k(NUM_SAMPLES, correct_count, 1)\n",
    "        p5 = pass_at_k(NUM_SAMPLES, correct_count, 5)\n",
    "        p10 = pass_at_k(NUM_SAMPLES, correct_count, 10)\n",
    "\n",
    "        results.append({\n",
    "            \"task_id\": task_ids[idx_in_batch],\n",
    "            \"correct_count\": correct_count,\n",
    "            \"pass@1\": p1,\n",
    "            \"pass@5\": p5,\n",
    "            \"pass@10\": p10,\n",
    "        })\n",
    "\n",
    "# Summarize\n",
    "df = pd.DataFrame(results)\n",
    "summary = {\n",
    "    \"tasks\": len(df),\n",
    "    \"avg_pass@1\": df[\"pass@1\"].mean(),\n",
    "    \"avg_pass@5\": df[\"pass@5\"].mean(),\n",
    "    \"avg_pass@10\": df[\"pass@10\"].mean()\n",
    "}\n",
    "print(\"SUMMARY:\", summary)\n",
    "\n",
    "# Save results\n",
    "out_csv = f\"{MODEL_NAME.replace('/', '_')}_{CONFIG_NAME}_planbench_results.csv\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "print(\"✅ Saved per-task results to\", out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qidtOuC9QcV3",
    "outputId": "fc2dfdce-bc81-4f5d-a0e5-ed747fa2e4c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG: TinyLlama/TinyLlama-1.1B-Chat-v1.0 NUM_SAMPLES= 5 BATCH_SIZE= 4 DEVICE= cuda\n",
      "Loading Defects4J dataset …\n",
      "✅ Loaded 467 bugs.\n",
      "Loading TinyLlama model …\n",
      "Model loaded successfully on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 117/117 [07:12<00:00,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done in 7.21 minutes. 467 bugs processed.\n",
      "SUMMARY: {'bugs': 467, 'avg_pass@1': np.float64(0.0), 'avg_pass@5': np.float64(0.0), 'avg_pass@10': np.float64(0.0)}\n",
      "Results saved to TinyLlama_TinyLlama-1.1B-Chat-v1.0_defects4j_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Colab-ready evaluation of Defects4J using TinyLlama ===\n",
    "# 0) Install dependencies\n",
    "!pip install -q transformers datasets accelerate torch tqdm\n",
    "\n",
    "# 1) Imports\n",
    "import os, json, math, tempfile, subprocess, sys, time\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# 2) Config\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "NUM_SAMPLES = 5       # number of completions per bug\n",
    "MAX_NEW_TOKENS = 128\n",
    "BATCH_SIZE = 4        # adjust for GPU\n",
    "TEMPERATURE = 0.8\n",
    "TOP_P = 0.95\n",
    "EVAL_TIMEOUT = 4      # seconds per evaluation\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"CONFIG:\", MODEL_NAME, \"NUM_SAMPLES=\", NUM_SAMPLES, \"BATCH_SIZE=\", BATCH_SIZE, \"DEVICE=\", DEVICE)\n",
    "\n",
    "# 3) Safe evaluation function\n",
    "def eval_code_with_test(code_str: str, test_str: str, timeout: int = EVAL_TIMEOUT) -> bool:\n",
    "    combined = code_str + \"\\n\\n\" + test_str + \"\\n\"\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".java\" if \"class\" in code_str else \".py\", delete=False) as tf:\n",
    "        tf.write(combined)\n",
    "        fname = tf.name\n",
    "    try:\n",
    "        proc = subprocess.run([sys.executable, fname],\n",
    "                              stdout=subprocess.PIPE,\n",
    "                              stderr=subprocess.PIPE,\n",
    "                              timeout=timeout)\n",
    "        return proc.returncode == 0\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return False\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(fname)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# 4) pass@k calculator\n",
    "def pass_at_k(n:int, c:int, k:int) -> float:\n",
    "    if c == 0: return 0.0\n",
    "    if k > n: k = n\n",
    "    try:\n",
    "        return 1.0 - math.comb(n - c, k) / math.comb(n, k)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "# 5) Load Defects4J dataset\n",
    "print(\"Loading Defects4J dataset …\")\n",
    "dataset = load_dataset(\"CoQuIR/Defects4J\", split=\"test\")  # split might be train/test\n",
    "print(f\"✅ Loaded {len(dataset)} bugs.\")\n",
    "\n",
    "# 6) Load TinyLlama model\n",
    "print(\"Loading TinyLlama model …\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if DEVICE == \"cuda\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully on\", DEVICE)\n",
    "\n",
    "# 7) Evaluation function\n",
    "def evaluate_defects4j(dataset, batch_size=BATCH_SIZE, num_samples=NUM_SAMPLES, max_new_tokens=MAX_NEW_TOKENS):\n",
    "    results = []\n",
    "    n_total = len(dataset)\n",
    "\n",
    "    # Convert to list of dicts to avoid indexing issues\n",
    "    dataset_list = [dict(d) for d in dataset]\n",
    "\n",
    "    for i in tqdm(range(0, n_total, batch_size), desc=\"Batches\"):\n",
    "        batch = dataset_list[i:i+batch_size]\n",
    "        # Build prompts from buggy code\n",
    "        prompts = [b.get(\"buggy_code\",\"\") for b in batch]\n",
    "        fixes = [b.get(\"fixed_code\",\"\") for b in batch]  # ground truth\n",
    "        task_ids = [b.get(\"bug_id\", idx+i) for idx,b in enumerate(batch)]\n",
    "\n",
    "        # tokenize\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=True,\n",
    "                temperature=TEMPERATURE,\n",
    "                top_p=TOP_P,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                num_return_sequences=num_samples,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        # Process each bug\n",
    "        for idx_in_batch, bug in enumerate(batch):\n",
    "            start = idx_in_batch * num_samples\n",
    "            end = start + num_samples\n",
    "            completions = decoded[start:end]\n",
    "            correct_count = 0\n",
    "            for gen_code in completions:\n",
    "                # Combine buggy code + generated fix for evaluation\n",
    "                full_code = prompts[idx_in_batch] + \"\\n\" + gen_code\n",
    "                # Simple check: does it match ground truth (for research / debugging)\n",
    "                if fixes[idx_in_batch] and fixes[idx_in_batch].strip() in full_code:\n",
    "                    correct_count += 1\n",
    "            # pass@k\n",
    "            p1 = pass_at_k(num_samples, correct_count, 1)\n",
    "            p5 = pass_at_k(num_samples, correct_count, 5)\n",
    "            p10 = pass_at_k(num_samples, correct_count, 10)\n",
    "\n",
    "            results.append({\n",
    "                \"bug_id\": task_ids[idx_in_batch],\n",
    "                \"correct_count\": correct_count,\n",
    "                \"pass@1\": p1,\n",
    "                \"pass@5\": p5,\n",
    "                \"pass@10\": p10\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "# 8) Run evaluation\n",
    "start_time = time.time()\n",
    "results = evaluate_defects4j(dataset)\n",
    "elapsed = (time.time() - start_time)/60\n",
    "print(f\"✅ Evaluation done in {elapsed:.2f} minutes. {len(results)} bugs processed.\")\n",
    "\n",
    "# 9) Save results\n",
    "df = pd.DataFrame(results)\n",
    "summary = {\n",
    "    \"bugs\": len(df),\n",
    "    \"avg_pass@1\": df[\"pass@1\"].mean(),\n",
    "    \"avg_pass@5\": df[\"pass@5\"].mean(),\n",
    "    \"avg_pass@10\": df[\"pass@10\"].mean()\n",
    "}\n",
    "print(\"SUMMARY:\", summary)\n",
    "out_csv = f\"{MODEL_NAME.replace('/','_')}_defects4j_results.csv\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "print(\"Results saved to\", out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "867c15af510548bcb1fab2f0f79d4ec6",
      "18292bfae76043118ef30170571d5e97",
      "e94c81de594043168d7b7ab407592c84",
      "c36f99b979b247349fa114ae8c7bb735",
      "7f86fc6ecc40429a8f6024aad15fb150",
      "531e5b91fb4244c3847346f38edb1283",
      "06c2734ee7c34ad8b3edc5df658b33f5",
      "dac8e04519de4d45993cb79506ae28b5",
      "3548437b21a1433482ac717efb649c0d",
      "2765ff09cf87401e81526e0c4b7ccfc7",
      "950c5922f99048d8b04e8fb878eeae29",
      "9ce109e3196c4d0ab9c14e33e07915b5",
      "708485b09d1a41ffac05af354e9c8277",
      "188646327c104ce2a091d5dcbce1ed62",
      "3cf34c749a9840da8767f4ee4748cd50",
      "c033836a7535404490c0ef576e6da575",
      "99341f496e474a9f83c2acce69ffa056",
      "cd95889a187e45ada1d2f59a2b03b7f1",
      "01be7a45c24d41fcafa8a88c352835b8",
      "c01f8c6987b24ee881c4ad381ec782eb",
      "8214e834d5bc4c62a03a329984b820d8",
      "68fea7f9eaac4c4cadd69f48efe8d50c",
      "c7dac97b635f4edca91476cc6e7eb27a",
      "abdc87cfdc0947d6a39e99604eccde8e",
      "f63c75100e474e06b46ecae28384b89c",
      "ccaa7fb098234082ab44797b58a0f657",
      "535cf7da101c486e8d60f94aa1ebe24f",
      "a917bbc12a8a42fbbb9774f49a9155ae",
      "ba2ec1460bd94e6385624c25ad5e428c",
      "fa234814aba2475f87a7e8c2bf72500f",
      "c77ab48ac0914eeda59b78358dbe2d5a",
      "694f3dc06de74a578a593b8383164d22",
      "81bb6cfcd80347479e36b94b48d2da2c",
      "9de8af7e2ae54f58a119366278c4cf5c",
      "9ab84acc3be44f0fb4f12090b609fee7",
      "50a757c921054b6fa41a827c324333c6",
      "5ccf55cf6a4048eb86493b4d7ebcc192",
      "1a33d6b01da54403b2a215f1c4582dc1",
      "f3f16b4118b84a55b4b471cb21f634fb",
      "6ab55f6d632045aa8781bc0c1c090f73",
      "aa2314f587fa4f4b80a930de17524ac0",
      "a20797a377b0425f8747aaf789eb09ce",
      "1fb486788f2e4935a10f64137cb3eb06",
      "6279dff61ef94988b4996c2b7743740e",
      "02419395ab4c420385527982d73634c0",
      "e4960e437124463cb650c43b00716ca3",
      "170605882c584efe8daccea7227692af",
      "890c9e54936f4f73b36d1336de364f9b",
      "c004677301bc4fc59969f0964f3b6310",
      "4deb83d237734afba862d462016c1360",
      "a929f11491f24ee59c07cd67c023b8bb",
      "05b2ace21eb145ac86585a108d0c86c3",
      "e1ea4af9b30047e0bcd28e9f78dce621",
      "f3333b33a06e4878be73da26a027568e",
      "5708f9e8f7004f28b713008798e6573e",
      "4b63dc45a13f474991d17c7fb2852e7d",
      "400aebf83f5a434891405b544898cf55",
      "2fe68273fec64cdf9c11cecee699f314",
      "fe621468901f458385ba75e397eccf1d",
      "a35be0ccd92444c8880f3793c81e62ef",
      "1f7aa3c5ac2b4cc1967593425c011a3c",
      "00c8adb76193469f964c3d2c36713da5",
      "70c609860fa94ed9ac44cb2740463827",
      "6233689f147848628aa72cc65df9ebf9",
      "935e5a3e6d10471db5156fbac2fcd899",
      "d84a44194b9c4246b4d823647b64e7be",
      "0da85f3784f74bcaa7be5477ab825473",
      "3d7b33decedb4fe0bd214111d4a68651",
      "042ee87673134e9496adfeca0d90c11c",
      "a77d49f6cb75488587351cb6229b76dd",
      "c00eaf63354b4c1bb0d47854b3820202",
      "012d45af6ed14657be95d070c83f9001",
      "72d460fcd1ec41f482d57c95594a5fa8",
      "a98dcca1bae942468062762ed51a6ded",
      "bfe96e981fa3442fbbaa262f745d3a71",
      "5871719c461a46f199b50288ab582cd0",
      "39c6d0c6f9364d9ea7b4a7d56d658c7e",
      "350e688dbdf449c78a0fa35d15cc2413",
      "596bed352e6742caa547d081a5e6a695",
      "bd83216e46534742a1853dbcb8f5e051",
      "0cc429018d954b2d8913389696c3c409",
      "75bb706faa0a4f93bcf5ba669df6a294",
      "c86cc521ebde437691d55f90f6fce6b9",
      "0595d59b6ac8480380ea15c62c485334",
      "fab95a85fe4340b9814616f387976224",
      "08a784c57c07464eb90977aa3cb60d54",
      "854303e16f284d979c6614673295dd5c",
      "355531ed536c41b58afbae2cfb995b47",
      "ab02f81f46024adcac988981ffa4b770",
      "fb754cdc757c4e7882b2938c6f069942",
      "1f76593594e14128baf78cd6d581e1df",
      "6e29b40bf30f4188a974f6d16df18e8e",
      "efdfd1bdd034432a84ed2202c1a25d23",
      "a5a9b7917e6140f992ad05d1432f867f",
      "9604e98df90e4db4ac7478bacb77e512",
      "247e45382415489aa39c90133e60f0ef",
      "6f67166378c442f1bb9397987bc02479",
      "08e80d93f4eb4d2fac86868793b09f53",
      "4c1b149c621f4f6a9f73c8ecfc508e89",
      "fc577d8d0f634650afb65d8aabe95fe3",
      "5d1400c5b28142b98a0f11980237b086",
      "b8ec148a0c0e42c6928b1508f2415655",
      "ee72d060f85e4ad59d63eedeb53cd47b",
      "daf82cb33da3475a84e950d08ab86c34",
      "2240bd617b1643129be37f9066a55fe4",
      "35b3545c112e433d8da0b3ee5a285cb4",
      "26f78a8ca882401a9d6cae9bd6e8b2d4",
      "ba24fe1e14a543578c2f9a9dd5191d85",
      "2e01430c1fc54d3b9533ff9a092bede9",
      "e3d0dda1640d4d1a914d1d9474f3a207",
      "6f9c937fcfcd42368fa061ae9fc5662e",
      "330b9f6f4a0045cbb98d4c3e1b39e4cc",
      "55308c57593e412494ee5702aed94875",
      "0c5ffa8568344431b3422f3fb4b92aa1",
      "05621330c26b44209de2932d6a37e82f",
      "158b83beaf844adfa663dca3208f0296",
      "3ee9ae720c0148e7a4687ec0b6151c89",
      "95d65339b3654d8cac3d88f7bc24c848",
      "0376d65e084c4ee485c62d92d0ac448c",
      "9c029c3d962a40c88e36879c300bdd23",
      "311a59198a5f404cbd9b0eb25fbbb808",
      "f386fe68b7cc49698e0f493e74e5eb88",
      "f3aab9fb817c495397adc46ce2241a54",
      "85a8e6018e48488690b0484489e39eb2",
      "508a6362112b422f9bce1d995f8d2255",
      "b7880c650800467a878038d9aeceb80d",
      "58dd9f3b75d542dbbf1bacf8ca6143b5",
      "85e03ef1b4644fecab71055c4e77f055",
      "155af30ef5004a0cae0b3b6a519bc9fc",
      "a0f84c0084934f6bb6f83b9cb4f04890",
      "2ef8fd5ee136423aaabb8d04732f43d1",
      "8b4878440e204dbebf7e641ca8b585d1",
      "228907ce7e4e491b99d74814b9e5cc0f",
      "cd6a97ce6cb9430786b2eda1ea0ad0d4",
      "497c7624569f4bfc9a0b7ca96535b86e",
      "2939cdfb68554139a55b22b9b4b63427",
      "3d6a606991d74297ab54834cb241988a",
      "0dc40f6ef9f744c999a137c3c616f411",
      "ae6ef018c3834c64bc235d3212933bef",
      "8a3b424870224aeb9c6f7ab97fe6621a",
      "348a7fa98f8d4b12803e6f6037419d47",
      "22ad57949b334bb289952180d358d849",
      "9e68b96716ae40b0a64003a272ef45a0",
      "158f9eb5c75c467eb7b1cbc4baf72866",
      "d4c68ae732ff4cebafb2861557a0c64b",
      "c68d050ddb504138a440ea43888120bf",
      "194b2be3b4d04aeb96b941b5a83c82c5",
      "64a1396eb168471eac601d8b69c4fdb2",
      "3d1822cef43640c683fa125abb7639f3",
      "59ef3566b1e84166bd76b130c7d753fa",
      "7965074b0d9747159a4a8ac84f151a9d",
      "c92ed9ec2f6b410b9654b55f000066f9",
      "0807192dc2214c96911a586790796145",
      "47e77adfc01344e29d47a6ec7d42730f",
      "245af8df813649b68a92b4282b792006",
      "e336305950dd4dc1942a88f3530498e6",
      "f50bef0ba58e4a3eb053aa9b8e5c3670",
      "c9ce974242884a7ea42c0c8fa4f701b9",
      "e19848d68cf14969935dbcf35c2e63f0",
      "8ea92b6b1e84469f98a2c37ac5b9fe68",
      "4dd5d30a76a94d16b21979edaaa1e9f4",
      "fcf0fb9c491943c684799cdeb9c1407a",
      "80752b186683423799e39fee3e6b952d",
      "2855ea653ea240f786c825c3285a1647",
      "5394c0d49293436eae8a7528a29945f5",
      "eeb6a52b449b4826a3b244bbd6e0339d",
      "57340ef04b794fa5a45b494c7b41d565",
      "5da44cc2496c432b92c9981cd558a699",
      "0579d59ecad547938abb9edf5a2346fc",
      "407905dc45eb4172855b816065e4abbf",
      "ad808fabe6964992b6f96dc6ad08f7c2",
      "41b725d8676448f3adee70e7c2acc3c9",
      "a75911be3b0d4db79eca6b380be6fff7",
      "e7c714e3fd934595854ce47636e3ff0a",
      "a857d1ec10c74ae698b10d75ca065c3c",
      "b8fd80bc8e734711809a4b2aee7d7ac4",
      "a5f28da21a874d72af77e60d4b7abaaa",
      "11fb439baa2f4adca4c532466db62cea",
      "68f78ed7c6794a46a420637e1921c753",
      "989ea0e6b874430fba1cdf75019852f3",
      "8689222a979e4d5390538b4152a0256b",
      "a177f08df5b64b87a13da0e27741d5f1",
      "0acb2081aa8b4342ac98335aa822df6c",
      "16e2d49179724053bc9f38336e05d2fc",
      "6fb662cdcf854c6f9332f5ef937ac646",
      "daac8b0ddbaa485296504ff20560c163",
      "ffdf6c1f59614f4ebc4bb4d7cb7a1c15",
      "e0b0d688348d4d22af6ea1f4c566b948",
      "04da15d397ac46cba591057014e8e3e6",
      "f9e92e7f979b4c8b805518190c851084",
      "dceb83743d3d480e88eaa83ebf7eabba",
      "094ca7fc8aac4002bd825f589cdf51b3",
      "aab524b76fd44b5c8f82fb5b5b496b10",
      "57778a7e65494be78f09c1bf3b157249",
      "d136d076e4ed4fb696eee48a3ac106bf",
      "228c85d4d761425da1de82d0720a9791",
      "88599ad787cc4eb8b40981b0bd260cc8",
      "51120ef213414e67a62a8cc70ed6a629"
     ]
    },
    "id": "Y9XAaXUfZLBf",
    "outputId": "b7663c77-47b5-4e9a-bab8-01ba9ff77381"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG: TinyLlama/TinyLlama-1.1B-Chat-v1.0 | DEVICE=cuda | BATCH=4 | NUM_SAMPLES=5\n",
      "Loading BigCodeBench dataset…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867c15af510548bcb1fab2f0f79d4ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce109e3196c4d0ab9c14e33e07915b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/v0.1.0_hf-00000-of-00001.parquet:   0%|          | 0.00/2.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7dac97b635f4edca91476cc6e7eb27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/v0.1.1-00000-of-00001.parquet:   0%|          | 0.00/2.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de8af7e2ae54f58a119366278c4cf5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/v0.1.2-00000-of-00001.parquet:   0%|          | 0.00/2.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02419395ab4c420385527982d73634c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/v0.1.3-00000-of-00001.parquet:   0%|          | 0.00/2.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b63dc45a13f474991d17c7fb2852e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/v0.1.4-00000-of-00001.parquet:   0%|          | 0.00/2.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da85f3784f74bcaa7be5477ab825473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating v0.1.0_hf split:   0%|          | 0/1140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350e688dbdf449c78a0fa35d15cc2413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating v0.1.1 split:   0%|          | 0/1140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab02f81f46024adcac988981ffa4b770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating v0.1.2 split:   0%|          | 0/1140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc577d8d0f634650afb65d8aabe95fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating v0.1.3 split:   0%|          | 0/1140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9c937fcfcd42368fa061ae9fc5662e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating v0.1.4 split:   0%|          | 0/1140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1140 tasks.\n",
      "Loading TinyLlama model…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f386fe68b7cc49698e0f493e74e5eb88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228907ce7e4e491b99d74814b9e5cc0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158f9eb5c75c467eb7b1cbc4baf72866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245af8df813649b68a92b4282b792006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb6a52b449b4826a3b244bbd6e0339d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f28da21a874d72af77e60d4b7abaaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b0d688348d4d22af6ea1f4c566b948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded. Device example param: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/285 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   0%|          | 1/285 [00:08<41:27,  8.76s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   1%|          | 2/285 [00:25<1:03:26, 13.45s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   1%|          | 3/285 [00:38<1:02:22, 13.27s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   1%|▏         | 4/285 [00:46<52:54, 11.30s/it]  A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   2%|▏         | 5/285 [00:53<45:46,  9.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   2%|▏         | 6/285 [01:01<41:50,  9.00s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   2%|▏         | 7/285 [01:08<38:53,  8.39s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   3%|▎         | 8/285 [01:17<40:00,  8.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   3%|▎         | 9/285 [01:25<37:52,  8.23s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   4%|▎         | 10/285 [01:33<38:03,  8.30s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   4%|▍         | 11/285 [01:42<38:36,  8.45s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   4%|▍         | 12/285 [01:50<37:42,  8.29s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   5%|▍         | 13/285 [01:59<38:48,  8.56s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   5%|▍         | 14/285 [02:07<37:50,  8.38s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   5%|▌         | 15/285 [02:13<34:57,  7.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   6%|▌         | 16/285 [02:22<35:40,  7.96s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   6%|▌         | 17/285 [02:31<37:48,  8.46s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   6%|▋         | 18/285 [02:39<36:11,  8.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   7%|▋         | 19/285 [02:49<38:29,  8.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   7%|▋         | 20/285 [02:58<39:53,  9.03s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   7%|▋         | 21/285 [03:07<39:14,  8.92s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   8%|▊         | 22/285 [03:19<42:42,  9.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   8%|▊         | 23/285 [03:28<41:48,  9.57s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   8%|▊         | 24/285 [03:38<42:10,  9.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   9%|▉         | 25/285 [03:45<39:08,  9.03s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   9%|▉         | 26/285 [03:54<37:46,  8.75s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:   9%|▉         | 27/285 [04:02<37:21,  8.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  10%|▉         | 28/285 [04:13<39:53,  9.31s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  10%|█         | 29/285 [04:21<37:48,  8.86s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  11%|█         | 30/285 [04:32<40:52,  9.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  11%|█         | 31/285 [04:43<41:54,  9.90s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  11%|█         | 32/285 [04:52<41:40,  9.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  12%|█▏        | 33/285 [05:01<39:53,  9.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  12%|█▏        | 34/285 [05:09<37:30,  8.96s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  12%|█▏        | 35/285 [05:16<35:14,  8.46s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  13%|█▎        | 36/285 [05:28<39:18,  9.47s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  13%|█▎        | 37/285 [05:35<36:41,  8.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  13%|█▎        | 38/285 [05:44<36:04,  8.76s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  14%|█▎        | 39/285 [05:52<35:03,  8.55s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  14%|█▍        | 40/285 [06:00<34:46,  8.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  14%|█▍        | 41/285 [06:10<36:04,  8.87s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  15%|█▍        | 42/285 [06:18<35:21,  8.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  15%|█▌        | 43/285 [06:27<35:02,  8.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  15%|█▌        | 44/285 [06:35<34:29,  8.59s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  16%|█▌        | 45/285 [06:44<34:03,  8.51s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  16%|█▌        | 46/285 [06:53<34:23,  8.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  16%|█▋        | 47/285 [07:04<37:01,  9.34s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  17%|█▋        | 48/285 [07:12<35:31,  8.99s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  17%|█▋        | 49/285 [07:22<36:35,  9.30s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  18%|█▊        | 50/285 [07:34<39:53, 10.19s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  18%|█▊        | 51/285 [07:43<38:23,  9.84s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  18%|█▊        | 52/285 [07:51<35:37,  9.18s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  19%|█▊        | 53/285 [07:59<34:43,  8.98s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  19%|█▉        | 54/285 [08:07<33:33,  8.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  19%|█▉        | 55/285 [08:17<34:11,  8.92s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  20%|█▉        | 56/285 [08:27<35:18,  9.25s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  20%|██        | 57/285 [08:36<34:53,  9.18s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  20%|██        | 58/285 [08:45<34:23,  9.09s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  21%|██        | 59/285 [08:53<33:27,  8.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  21%|██        | 60/285 [09:02<33:34,  8.95s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  21%|██▏       | 61/285 [09:11<32:43,  8.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  22%|██▏       | 62/285 [09:19<32:11,  8.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  22%|██▏       | 63/285 [09:28<32:34,  8.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  22%|██▏       | 64/285 [09:36<31:18,  8.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  23%|██▎       | 65/285 [09:46<32:30,  8.86s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  23%|██▎       | 66/285 [09:53<30:40,  8.40s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  24%|██▎       | 67/285 [10:01<30:33,  8.41s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  24%|██▍       | 68/285 [10:10<30:14,  8.36s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  24%|██▍       | 69/285 [10:18<30:36,  8.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  25%|██▍       | 70/285 [10:26<29:17,  8.18s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  25%|██▍       | 71/285 [10:34<29:03,  8.15s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  25%|██▌       | 72/285 [10:41<27:18,  7.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  26%|██▌       | 73/285 [10:49<27:56,  7.91s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  26%|██▌       | 74/285 [10:57<28:05,  7.99s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  26%|██▋       | 75/285 [11:06<29:05,  8.31s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  27%|██▋       | 76/285 [11:15<29:09,  8.37s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  27%|██▋       | 77/285 [11:23<29:01,  8.37s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  27%|██▋       | 78/285 [11:32<29:17,  8.49s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  28%|██▊       | 79/285 [11:40<28:47,  8.39s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  28%|██▊       | 80/285 [11:48<27:59,  8.19s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  28%|██▊       | 81/285 [11:57<29:03,  8.55s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  29%|██▉       | 82/285 [12:05<28:25,  8.40s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  29%|██▉       | 83/285 [12:13<27:40,  8.22s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  29%|██▉       | 84/285 [12:21<26:56,  8.04s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  30%|██▉       | 85/285 [12:29<27:02,  8.11s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  30%|███       | 86/285 [12:38<27:52,  8.41s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  31%|███       | 87/285 [12:46<27:20,  8.28s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  31%|███       | 88/285 [12:54<26:52,  8.19s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  31%|███       | 89/285 [13:03<27:48,  8.51s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  32%|███▏      | 90/285 [13:11<27:05,  8.34s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  32%|███▏      | 91/285 [13:21<28:33,  8.83s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  32%|███▏      | 92/285 [13:29<27:44,  8.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  33%|███▎      | 93/285 [13:37<26:58,  8.43s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  33%|███▎      | 94/285 [13:45<26:30,  8.33s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  33%|███▎      | 95/285 [13:54<26:29,  8.36s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  34%|███▎      | 96/285 [14:04<28:05,  8.92s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  34%|███▍      | 97/285 [14:14<29:17,  9.35s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  34%|███▍      | 98/285 [14:22<27:49,  8.93s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  35%|███▍      | 99/285 [14:31<27:31,  8.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  35%|███▌      | 100/285 [14:38<26:03,  8.45s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  35%|███▌      | 101/285 [14:47<25:50,  8.43s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  36%|███▌      | 102/285 [14:55<25:10,  8.25s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  36%|███▌      | 103/285 [15:04<26:25,  8.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  36%|███▋      | 104/285 [15:13<26:01,  8.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  37%|███▋      | 105/285 [15:23<27:19,  9.11s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  37%|███▋      | 106/285 [15:34<28:21,  9.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  38%|███▊      | 107/285 [15:45<30:00, 10.12s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  38%|███▊      | 108/285 [15:55<29:31, 10.01s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  38%|███▊      | 109/285 [16:05<29:21, 10.01s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  39%|███▊      | 110/285 [16:15<29:01,  9.95s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  39%|███▉      | 111/285 [16:25<29:03, 10.02s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  39%|███▉      | 112/285 [16:34<27:59,  9.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  40%|███▉      | 113/285 [16:45<28:38,  9.99s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  40%|████      | 114/285 [16:53<27:30,  9.65s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  40%|████      | 115/285 [17:02<26:21,  9.31s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  41%|████      | 116/285 [17:10<25:37,  9.10s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  41%|████      | 117/285 [17:19<25:01,  8.94s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  41%|████▏     | 118/285 [17:28<24:42,  8.87s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  42%|████▏     | 119/285 [17:37<25:15,  9.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  42%|████▏     | 120/285 [17:47<25:16,  9.19s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  42%|████▏     | 121/285 [17:56<25:08,  9.20s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  43%|████▎     | 122/285 [18:08<27:00,  9.94s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  43%|████▎     | 123/285 [18:18<27:11, 10.07s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  44%|████▎     | 124/285 [18:28<27:12, 10.14s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  44%|████▍     | 125/285 [18:38<26:42, 10.01s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  44%|████▍     | 126/285 [18:47<25:35,  9.66s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  45%|████▍     | 127/285 [18:56<25:14,  9.59s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  45%|████▍     | 128/285 [19:06<25:09,  9.61s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  45%|████▌     | 129/285 [19:16<25:21,  9.75s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  46%|████▌     | 130/285 [19:25<24:14,  9.39s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  46%|████▌     | 131/285 [19:35<24:28,  9.54s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  46%|████▋     | 132/285 [19:43<23:20,  9.15s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  47%|████▋     | 133/285 [19:52<23:17,  9.19s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  47%|████▋     | 134/285 [20:05<25:53, 10.29s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  47%|████▋     | 135/285 [20:13<24:05,  9.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  48%|████▊     | 136/285 [20:21<22:52,  9.21s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  48%|████▊     | 137/285 [20:28<21:04,  8.55s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  48%|████▊     | 138/285 [20:36<20:01,  8.17s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  49%|████▉     | 139/285 [20:44<20:04,  8.25s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  49%|████▉     | 140/285 [20:53<20:19,  8.41s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  49%|████▉     | 141/285 [21:01<19:53,  8.29s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  50%|████▉     | 142/285 [21:09<19:30,  8.18s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  50%|█████     | 143/285 [21:17<19:23,  8.19s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  51%|█████     | 144/285 [21:24<18:45,  7.98s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  51%|█████     | 145/285 [21:32<18:23,  7.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  51%|█████     | 146/285 [21:40<18:20,  7.92s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  52%|█████▏    | 147/285 [21:49<18:46,  8.17s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  52%|█████▏    | 148/285 [21:57<18:19,  8.03s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  52%|█████▏    | 149/285 [22:05<18:25,  8.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  53%|█████▎    | 150/285 [22:12<17:43,  7.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  53%|█████▎    | 151/285 [22:19<16:58,  7.60s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  53%|█████▎    | 152/285 [22:27<16:54,  7.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  54%|█████▎    | 153/285 [22:35<17:09,  7.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  54%|█████▍    | 154/285 [22:44<17:56,  8.22s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  54%|█████▍    | 155/285 [22:54<18:41,  8.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  55%|█████▍    | 156/285 [23:01<17:25,  8.11s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  55%|█████▌    | 157/285 [23:08<16:40,  7.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  55%|█████▌    | 158/285 [23:15<16:04,  7.60s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  56%|█████▌    | 159/285 [23:23<16:09,  7.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  56%|█████▌    | 160/285 [23:30<15:36,  7.49s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  56%|█████▋    | 161/285 [23:38<15:43,  7.61s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  57%|█████▋    | 162/285 [23:45<15:17,  7.46s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  57%|█████▋    | 163/285 [23:53<15:24,  7.58s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  58%|█████▊    | 164/285 [24:02<16:23,  8.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  58%|█████▊    | 165/285 [24:11<16:27,  8.23s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  58%|█████▊    | 166/285 [24:19<16:10,  8.15s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  59%|█████▊    | 167/285 [24:27<16:06,  8.19s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  59%|█████▉    | 168/285 [24:33<14:51,  7.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  59%|█████▉    | 169/285 [24:40<14:01,  7.25s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  60%|█████▉    | 170/285 [24:48<14:31,  7.58s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  60%|██████    | 171/285 [24:55<14:17,  7.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  60%|██████    | 172/285 [25:04<14:48,  7.86s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  61%|██████    | 173/285 [25:21<19:44, 10.58s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  61%|██████    | 174/285 [25:29<18:01,  9.75s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  61%|██████▏   | 175/285 [25:37<16:54,  9.23s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  62%|██████▏   | 176/285 [25:44<15:46,  8.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  62%|██████▏   | 177/285 [25:51<14:53,  8.28s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  62%|██████▏   | 178/285 [25:58<13:48,  7.75s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  63%|██████▎   | 179/285 [26:05<13:16,  7.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  63%|██████▎   | 180/285 [26:12<13:11,  7.54s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  64%|██████▎   | 181/285 [26:19<12:38,  7.29s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  64%|██████▍   | 182/285 [26:27<12:51,  7.49s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  64%|██████▍   | 183/285 [26:35<12:56,  7.61s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  65%|██████▍   | 184/285 [26:41<12:14,  7.27s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  65%|██████▍   | 185/285 [26:48<11:35,  6.95s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  65%|██████▌   | 186/285 [26:57<12:42,  7.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  66%|██████▌   | 187/285 [27:07<13:33,  8.30s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  66%|██████▌   | 188/285 [27:19<15:28,  9.58s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  66%|██████▋   | 189/285 [27:30<15:40,  9.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  67%|██████▋   | 190/285 [27:40<15:39,  9.89s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  67%|██████▋   | 191/285 [27:59<19:44, 12.60s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  67%|██████▋   | 192/285 [28:19<22:55, 14.79s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  68%|██████▊   | 193/285 [28:28<20:16, 13.22s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  68%|██████▊   | 194/285 [28:37<18:09, 11.97s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  68%|██████▊   | 195/285 [28:49<17:44, 11.83s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  69%|██████▉   | 196/285 [28:59<16:49, 11.34s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  69%|██████▉   | 197/285 [29:11<16:50, 11.49s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  69%|██████▉   | 198/285 [29:20<15:45, 10.87s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  70%|██████▉   | 199/285 [29:30<15:02, 10.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  70%|███████   | 200/285 [29:41<15:01, 10.61s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  71%|███████   | 201/285 [29:50<14:12, 10.15s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  71%|███████   | 202/285 [29:59<13:33,  9.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  71%|███████   | 203/285 [30:08<13:18,  9.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  72%|███████▏  | 204/285 [30:22<14:51, 11.00s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  72%|███████▏  | 205/285 [30:33<14:28, 10.85s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  72%|███████▏  | 206/285 [30:40<12:59,  9.86s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  73%|███████▎  | 207/285 [30:48<12:01,  9.25s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  73%|███████▎  | 208/285 [30:57<11:43,  9.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  73%|███████▎  | 209/285 [31:07<11:51,  9.36s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  74%|███████▎  | 210/285 [31:17<11:59,  9.60s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  74%|███████▍  | 211/285 [31:29<12:33, 10.18s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  74%|███████▍  | 212/285 [31:40<12:52, 10.59s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  75%|███████▍  | 213/285 [31:49<12:09, 10.13s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  75%|███████▌  | 214/285 [31:58<11:19,  9.57s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  75%|███████▌  | 215/285 [32:08<11:19,  9.71s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  76%|███████▌  | 216/285 [32:17<11:12,  9.75s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  76%|███████▌  | 217/285 [32:29<11:33, 10.19s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  76%|███████▋  | 218/285 [32:40<11:45, 10.53s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  77%|███████▋  | 219/285 [32:50<11:32, 10.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  77%|███████▋  | 220/285 [33:03<12:00, 11.09s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  78%|███████▊  | 221/285 [33:16<12:37, 11.83s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  78%|███████▊  | 222/285 [33:32<13:41, 13.04s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  78%|███████▊  | 223/285 [33:41<12:05, 11.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  79%|███████▊  | 224/285 [33:48<10:29, 10.33s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  79%|███████▉  | 225/285 [33:56<09:43,  9.72s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  79%|███████▉  | 226/285 [34:05<09:23,  9.55s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  80%|███████▉  | 227/285 [34:13<08:35,  8.89s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  80%|████████  | 228/285 [34:21<08:23,  8.83s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  80%|████████  | 229/285 [34:30<08:13,  8.81s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  81%|████████  | 230/285 [34:40<08:14,  9.00s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  81%|████████  | 231/285 [34:51<08:37,  9.58s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  81%|████████▏ | 232/285 [34:58<07:56,  8.99s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  82%|████████▏ | 233/285 [35:06<07:24,  8.55s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  82%|████████▏ | 234/285 [35:13<07:04,  8.32s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  82%|████████▏ | 235/285 [35:21<06:39,  8.00s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  83%|████████▎ | 236/285 [35:31<07:12,  8.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  83%|████████▎ | 237/285 [35:42<07:29,  9.36s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  84%|████████▎ | 238/285 [35:50<07:05,  9.06s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  84%|████████▍ | 239/285 [36:00<06:59,  9.11s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  84%|████████▍ | 240/285 [36:07<06:23,  8.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  85%|████████▍ | 241/285 [36:15<06:05,  8.31s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  85%|████████▍ | 242/285 [36:23<05:56,  8.29s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  85%|████████▌ | 243/285 [36:31<05:46,  8.25s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  86%|████████▌ | 244/285 [36:39<05:37,  8.22s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  86%|████████▌ | 245/285 [36:50<05:55,  8.89s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  86%|████████▋ | 246/285 [36:59<05:47,  8.92s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  87%|████████▋ | 247/285 [37:08<05:47,  9.14s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  87%|████████▋ | 248/285 [37:18<05:44,  9.30s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  87%|████████▋ | 249/285 [37:28<05:40,  9.47s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  88%|████████▊ | 250/285 [37:36<05:21,  9.17s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  88%|████████▊ | 251/285 [37:44<04:56,  8.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  88%|████████▊ | 252/285 [37:53<04:53,  8.90s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  89%|████████▉ | 253/285 [38:04<05:01,  9.42s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  89%|████████▉ | 254/285 [38:13<04:46,  9.23s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  89%|████████▉ | 255/285 [38:22<04:37,  9.25s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  90%|████████▉ | 256/285 [38:31<04:23,  9.08s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  90%|█████████ | 257/285 [38:41<04:25,  9.49s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  91%|█████████ | 258/285 [38:49<04:03,  9.01s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  91%|█████████ | 259/285 [38:59<04:01,  9.29s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  91%|█████████ | 260/285 [39:09<03:57,  9.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  92%|█████████▏| 261/285 [39:19<03:51,  9.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  92%|█████████▏| 262/285 [39:29<03:41,  9.62s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  92%|█████████▏| 263/285 [39:37<03:25,  9.35s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  93%|█████████▎| 264/285 [39:48<03:24,  9.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  93%|█████████▎| 265/285 [40:02<03:38, 10.94s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  93%|█████████▎| 266/285 [40:12<03:24, 10.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  94%|█████████▎| 267/285 [40:22<03:11, 10.63s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  94%|█████████▍| 268/285 [40:31<02:52, 10.16s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  94%|█████████▍| 269/285 [40:40<02:36,  9.75s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  95%|█████████▍| 270/285 [40:51<02:32, 10.18s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  95%|█████████▌| 271/285 [41:01<02:21, 10.12s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  95%|█████████▌| 272/285 [41:10<02:05,  9.68s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  96%|█████████▌| 273/285 [41:18<01:51,  9.29s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  96%|█████████▌| 274/285 [41:26<01:35,  8.67s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  96%|█████████▋| 275/285 [41:36<01:32,  9.23s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  97%|█████████▋| 276/285 [41:46<01:26,  9.57s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  97%|█████████▋| 277/285 [41:55<01:13,  9.18s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  98%|█████████▊| 278/285 [42:03<01:02,  8.96s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  98%|█████████▊| 279/285 [42:12<00:53,  8.93s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  98%|█████████▊| 280/285 [42:21<00:44,  8.94s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  99%|█████████▊| 281/285 [42:31<00:37,  9.29s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  99%|█████████▉| 282/285 [42:39<00:26,  8.78s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches:  99%|█████████▉| 283/285 [42:47<00:17,  8.58s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches: 100%|█████████▉| 284/285 [42:55<00:08,  8.37s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Batches: 100%|██████████| 285/285 [43:03<00:00,  9.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation finished in 43.06 minutes for 1140 tasks.\n",
      "SUMMARY: {'tasks': 1140, 'avg_pass@1': np.float64(0.0), 'avg_pass@5': np.float64(0.0), 'avg_pass@10': np.float64(0.0)}\n",
      "Saved results to TinyLlama_TinyLlama-1.1B-Chat-v1.0_bigcodebench_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Colab-ready BigCodeBench evaluation ===\n",
    "# 0) Install dependencies (first run)\n",
    "!pip install -q transformers datasets accelerate torch tqdm\n",
    "\n",
    "# 1) Imports\n",
    "import os, json, math, tempfile, subprocess, sys, time\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# 2) Config\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "NUM_SAMPLES = 5          # number of completions per prompt\n",
    "MAX_NEW_TOKENS = 128     # max tokens to generate per completion\n",
    "BATCH_SIZE = 4\n",
    "TEMPERATURE = 0.8\n",
    "TOP_P = 0.95\n",
    "EVAL_TIMEOUT = 5         # timeout for each code execution\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"CONFIG: {MODEL_NAME} | DEVICE={DEVICE} | BATCH={BATCH_SIZE} | NUM_SAMPLES={NUM_SAMPLES}\")\n",
    "\n",
    "# 3) Safe evaluation function\n",
    "def eval_code_with_test(code_str: str, test_str: str, timeout: int = EVAL_TIMEOUT) -> bool:\n",
    "    combined = code_str + \"\\n\\n\" + test_str\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as tf:\n",
    "        tf.write(combined)\n",
    "        fname = tf.name\n",
    "    try:\n",
    "        proc = subprocess.run([sys.executable, fname],\n",
    "                              stdout=subprocess.PIPE,\n",
    "                              stderr=subprocess.PIPE,\n",
    "                              timeout=timeout)\n",
    "        return proc.returncode == 0\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return False\n",
    "    finally:\n",
    "        try: os.remove(fname)\n",
    "        except: pass\n",
    "\n",
    "# 4) pass@k metric\n",
    "def pass_at_k(n:int, c:int, k:int) -> float:\n",
    "    if c == 0: return 0.0\n",
    "    if k > n: k = n\n",
    "    try:\n",
    "        return 1.0 - math.comb(n - c, k) / math.comb(n, k)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "# 5) Load BigCodeBench dataset\n",
    "print(\"Loading BigCodeBench dataset…\")\n",
    "dataset = load_dataset(\"bigcode/bigcodebench\", split=\"v0.1.4\")  # full dataset\n",
    "dataset = list(dataset)  # convert to list of dicts for easier batching\n",
    "print(f\"✅ Loaded {len(dataset)} tasks.\")\n",
    "\n",
    "# 6) Load TinyLlama model\n",
    "print(\"Loading TinyLlama model…\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if DEVICE==\"cuda\" else torch.float32,\n",
    "    device_map=\"auto\" if DEVICE==\"cuda\" else None\n",
    ")\n",
    "model.eval()\n",
    "print(\"✅ Model loaded. Device example param:\", next(model.parameters()).device)\n",
    "\n",
    "# 7) Evaluation loop (batched + safe execution)\n",
    "def evaluate_bigcodebench(dataset, batch_size=BATCH_SIZE, num_samples=NUM_SAMPLES, max_new_tokens=MAX_NEW_TOKENS):\n",
    "    results = []\n",
    "    n_total = len(dataset)\n",
    "\n",
    "    for i in tqdm(range(0, n_total, batch_size), desc=\"Batches\"):\n",
    "        batch = dataset[i:i+batch_size]\n",
    "\n",
    "        prompts = [item.get(\"complete_prompt\") or item.get(\"instruct_prompt\") for item in batch]\n",
    "        task_ids = [item.get(\"task_id\", idx+i) for idx,item in enumerate(batch)]\n",
    "        solutions = [item.get(\"canonical_solution\", \"\") for item in batch]\n",
    "\n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(next(model.parameters()).device)\n",
    "\n",
    "        # Generate multiple completions per prompt\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                do_sample=True,\n",
    "                temperature=TEMPERATURE,\n",
    "                top_p=TOP_P,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                num_return_sequences=num_samples,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        # Process completions\n",
    "        for idx_in_batch, task_id in enumerate(task_ids):\n",
    "            start = idx_in_batch*num_samples\n",
    "            end = start+num_samples\n",
    "            completions = decoded[start:end]\n",
    "            correct_count = 0\n",
    "\n",
    "            for c in completions:\n",
    "                gen_code = c\n",
    "                # Combine with prompt if model returned only completion\n",
    "                if gen_code.startswith(prompts[idx_in_batch]):\n",
    "                    gen_code = gen_code[len(prompts[idx_in_batch]):].lstrip(\"\\n\")\n",
    "                full_code = prompts[idx_in_batch] + \"\\n\" + gen_code\n",
    "\n",
    "                # Safe execution: compare with canonical solution if available\n",
    "                test_code = f\"assert {solutions[idx_in_batch].strip()} == {gen_code.strip()}\" if solutions[idx_in_batch] else \"\"\n",
    "                try:\n",
    "                    if test_code:\n",
    "                        if eval_code_with_test(\"\", test_code, timeout=EVAL_TIMEOUT):\n",
    "                            correct_count += 1\n",
    "                    else:\n",
    "                        # fallback heuristic: look for function definitions or return statements\n",
    "                        if (\"def \" in gen_code) or (\"return\" in gen_code):\n",
    "                            correct_count += 1\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            results.append({\n",
    "                \"task_id\": task_id,\n",
    "                \"correct_count\": correct_count,\n",
    "                \"num_samples\": num_samples,\n",
    "                \"pass@1\": pass_at_k(num_samples, correct_count, 1),\n",
    "                \"pass@5\": pass_at_k(num_samples, correct_count, 5),\n",
    "                \"pass@10\": pass_at_k(num_samples, correct_count, 10)\n",
    "            })\n",
    "\n",
    "        # small sleep to avoid GPU throttling\n",
    "        time.sleep(0.05)\n",
    "\n",
    "    return results\n",
    "\n",
    "# 8) Run evaluation\n",
    "start_time = time.time()\n",
    "results = evaluate_bigcodebench(dataset, batch_size=BATCH_SIZE, num_samples=NUM_SAMPLES, max_new_tokens=MAX_NEW_TOKENS)\n",
    "elapsed = time.time()-start_time\n",
    "print(f\"✅ Evaluation finished in {elapsed/60:.2f} minutes for {len(results)} tasks.\")\n",
    "\n",
    "# 9) Save summary & per-task results\n",
    "df = pd.DataFrame(results)\n",
    "summary = {\n",
    "    \"tasks\": len(df),\n",
    "    \"avg_pass@1\": df[\"pass@1\"].mean(),\n",
    "    \"avg_pass@5\": df[\"pass@5\"].mean(),\n",
    "    \"avg_pass@10\": df[\"pass@10\"].mean()\n",
    "}\n",
    "print(\"SUMMARY:\", summary)\n",
    "\n",
    "out_csv = f\"{MODEL_NAME.replace('/','_')}_bigcodebench_results.csv\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "print(\"Saved results to\", out_csv)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
